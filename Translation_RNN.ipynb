{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Translation English Indonesia (RNN) -> Evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masalah yang ingin diselesaikan \n",
    "Masalah translasi teks dari Bahasa Inggris ke Indonesia "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data yang Dipilih\n",
    "\n",
    "Data yang kami dapatkan dari https://github.com/lppier/Seq2Seq_Eng2Indo-Translation, tetapi karena awalnya masih kurang jadi kami menambahkan dataset tambahan https://github.com/prasastoadi/parallel-corpora-en-id/ (IWSLT17.TED.tst2017plus.en-id.tok, PANL-BPPT-INT-EN-150Kw.tok, PANL-BPPT-SPO-EN-100Kw.tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arsitektur dan Model yang Dipilih\n",
    "- Arsitektur: RNN\n",
    "- Model: Seq 2 Seq "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n"
     ]
    }
   ],
   "source": [
    "MIN_LENGTH = 4 # define min words\n",
    "MAX_LENGTH = 102 # define max words\n",
    "START, START_IDX = '<s>',  0\n",
    "END, END_IDX = '</s>', 1\n",
    "UNK, UNK_IDX = 'UNK', 2\n",
    "\n",
    "SOS_token = START_IDX\n",
    "EOS_token = END_IDX\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Lets load our dictionaries.\n",
    "f_eng = open('vocabs/simple_english_vocab.Dictionary.pkl', 'rb')\n",
    "english_vocab = pickle.load(f_eng)\n",
    "\n",
    "f_ind = open('vocabs/simple_indo_vocab.Dictionary.pkl', 'rb')\n",
    "indo_vocab = pickle.load(f_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penjelasan:\n",
    "Pada bagian ini kita akan melakukan setting untuk tokenization dari input yang ada dan kita akan load dictionary tokenizer yang kita dapat dari training sebelumnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define Encoder and Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):  # Add max_length as a parameter\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length  # Update max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)  # Update attention layer\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penjelasan:\n",
    "EncoderRNN memproses input menggunakan embedding dan GRU (Gated Recurrent Unit), sedangkan AttnDecoderRNN berfungsi sebagai decoder dengan mekanisme attention untuk menghasilkan output berdasarkan input, hidden state, dan output dari encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Sentences to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#mengonversi kalimat menjadi vektor \n",
    "def vectorize_sent(sent, vocab):\n",
    "    return vocab.doc2idx([START] + word_tokenize(sent.lower()) + [END], unknown_word_index=2)\n",
    "\n",
    "#mengonversi vektor ke dalam bentuk tensor pytorch untuk input model \n",
    "def variable_from_sent(sent, vocab):\n",
    "    vsent = vectorize_sent(sent, vocab)\n",
    "    # print(vsent)\n",
    "    result = Variable(torch.LongTensor(vsent).view(-1, 1))\n",
    "    # print(result)\n",
    "    return result.cuda() if use_cuda else result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#lakukan evaluasi pada model\n",
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        # input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_tensor = variable_from_sent(sentence, english_vocab)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('</s>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(indo_vocab.id2token[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "encoder = EncoderRNN(len(english_vocab), hidden_size).to(device)\n",
    "attn_decoder = AttnDecoderRNN(hidden_size, len(indo_vocab), dropout_p=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Load a network weight snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderRNN(\n",
       "  (embedding): Embedding(18990, 512)\n",
       "  (gru): GRU(512, 512)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENCODER_PATH = 'results/encoder-{}.pth'\n",
    "DECODER_PATH = 'results/decoder-{}.pth'\n",
    "\n",
    "EPOCH_NO = 75000\n",
    "\n",
    "encoder.load_state_dict(torch.load(ENCODER_PATH.format(EPOCH_NO), map_location=device))\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(18722, 512)\n",
       "  (attn): Linear(in_features=1024, out_features=102, bias=True)\n",
       "  (attn_combine): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (gru): GRU(512, 512)\n",
       "  (out): Linear(in_features=512, out_features=18722, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_decoder.load_state_dict(torch.load(DECODER_PATH.format(EPOCH_NO),map_location=device))\n",
    "attn_decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Mini Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Tom is a good man\n",
      "< <s> tom adalah seorang yang baik . </s>\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Tom is a good man\"\n",
    "output_words, attentions = evaluate(encoder, attn_decoder, input_sentence)\n",
    "output_sentence = ' '.join(output_words)\n",
    "print('>', input_sentence)\n",
    "print('<', output_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Load an array of network weights snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "ENCODER_PATH = 'results/encoder-{}.pth'\n",
    "DECODER_PATH = 'results/decoder-{}.pth'\n",
    "\n",
    "EPOCH_NO = [75000]\n",
    "\n",
    "def load_weights(encoder, decoder, epoch_no, device):\n",
    "    encoder.load_state_dict(torch.load(ENCODER_PATH.format(epoch_no),map_location=device))\n",
    "    decoder.load_state_dict(torch.load(DECODER_PATH.format(epoch_no),map_location=device))\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "# Input Validation to make sure if the inputs between the min length and max length\n",
    "def input_validation(input_text, english_vocab):\n",
    "    max_words_required = MAX_LENGTH - 2\n",
    "    min_words_required = MIN_LENGTH\n",
    "    input_tokenized = word_tokenize(input_text.lower())\n",
    "    final_text = None\n",
    "    message = \"\"\n",
    "    if not min_words_required <= len(input_tokenized) <= max_words_required:\n",
    "        message = \"The input sentence should be between {} and {} words\".format(MIN_LENGTH, MAX_LENGTH - 2)\n",
    "    else:\n",
    "        input_ids = english_vocab.doc2idx(input_tokenized)\n",
    "        unknown_tokens = []\n",
    "        for key, val in enumerate(input_ids):\n",
    "            if val == -1:\n",
    "                unknown_token = input_tokenized[key]\n",
    "                unknown_tokens.append(unknown_token)\n",
    "                input_tokenized[key] = UNK.lower()\n",
    "                print(\"'{}' is not found in the english corpus\".format(unknown_token))\n",
    "        final_text = \" \".join(input_tokenized)\n",
    "    return final_text, message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: ipywidgets in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (8.0.4)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipywidgets) (6.25.0)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipywidgets) (8.12.2)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipywidgets) (4.0.5)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipywidgets) (3.0.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.3.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=20 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets) (6.3.2)\n",
      "Requirement already satisfied: backcall in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.5.1->ipywidgets) (305.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\elisa\\anaconda3\\envs\\deeplearning\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95ae7660ca242f0885f8bcaabea6cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='English:', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cbd2b7e11e426fb85c6687348b1261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='info', description='Translate', icon='check', style=ButtonStyle(), tooltip='Translate')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "input_text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type something',\n",
    "    description='English:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='Translate',\n",
    "    disabled=False,\n",
    "    button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Translate',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    eng2indo_translation(input_text.value, encoder, attn_decoder, english_vocab)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "display(input_text)\n",
    "display(button)\n",
    "\n",
    "def eng2indo_translation(text, encoder, decoder, english_vocab):\n",
    "    sentences = text.split('.')  # Split input text into sentences based on \".\"\n",
    "    translated_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        final_text, message = input_validation(sentence.strip(), english_vocab)\n",
    "\n",
    "        if final_text:\n",
    "            print('>', sentence)\n",
    "            print('>>', final_text)\n",
    "\n",
    "            for epoch in EPOCH_NO:\n",
    "                encoder, decoder = load_weights(encoder, decoder, epoch, device)\n",
    "                output_words, attentions = evaluate(encoder, decoder, final_text)\n",
    "                output_sentence = ' '.join(output_words).replace('<s>', '').replace('</s>', '')\n",
    "                translated_sentences.append(output_sentence)\n",
    "\n",
    "    # Concatenate translated sentences\n",
    "    final_output = ' '.join(translated_sentences)\n",
    "    print(\"Final Translation:\", final_output)\n",
    "\n",
    "    if message:\n",
    "        print(\"Validation Message:\", message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> she is standing there \n",
      ">> she is standing there\n",
      "Final Translation:  dia ada di sana . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "> he is a bad man \n",
      ">> he is a bad man\n",
      "Final Translation:  dia adalah seorang yang yang sangat \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "> he wants to sleep \n",
      ">> he wants to sleep\n",
      "Final Translation:  dia ingin tidak lagi . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "'n't' is not found in the english corpus\n",
      "> i can't see you crying \n",
      ">> i ca unk see you crying\n",
      "Final Translation:  aku sudah makan kamu kamu . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "> my dog is running around \n",
      ">> my dog is running around\n",
      "Final Translation:  saya saya berada di luar . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "> it is very popular \n",
      ">> it is very popular\n",
      "Final Translation:  ini benar benar . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "''s' is not found in the english corpus\n",
      "> she speaks american english to tom's father \n",
      ">> she speaks american english to tom unk father\n",
      "Final Translation:  dia sudah amerika amerika amerika tom telah \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "> please eat lunch in the afternoon \n",
      ">> please eat lunch in the afternoon\n",
      "Final Translation:  tolong makan makan di depan . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n",
      "> i see red roses in the garden \n",
      ">> i see red roses in the garden\n",
      "Final Translation:  saya melihat melihat di di taman . \n",
      "Validation Message: The input sentence should be between 4 and 100 words\n"
     ]
    }
   ],
   "source": [
    "eng2indo_translation(\"she is standing there .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"he is a bad man .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"he wants to sleep .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"i can't see you crying .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"my dog is running around .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"it is very popular .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"she speaks american english to tom's father .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"please eat lunch in the afternoon .\", encoder, attn_decoder, english_vocab)\n",
    "eng2indo_translation(\"i see red roses in the garden .\", encoder, attn_decoder, english_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Komitmen Integritas\n",
    "Di hadapan TUHAN yang hidup, saya menegaskan bahwa saya tidak memberikan maupun\n",
    "menerima bantuan apapun—baik lisan, tulisan, maupun elektronik—di dalam ujian ini selain\n",
    "daripada apa yang telah diizinkan oleh pengajar, dan tidak akan menyebarkan baik soal\n",
    "maupun jawaban ujian kepada pihak lain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
